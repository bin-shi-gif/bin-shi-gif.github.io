<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Bio</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<h2>Journal Publications </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2407.18681">A Lyapunov Analysis of Accelerated PDHG Algorithms</a>, To appear in <i>Journal of Optimization Theory and Applications, 2025+.</i></p>
<ul>
<li><p>Xueying Zeng and <b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0021999125004462">Numerical Solution for Nonlinear 4D Variational Data Assimilation (4D-Var) via ADMM</a>,&nbsp;&nbsp;[<a href="https://github.com/bowenmath/4D-Var-via-ADMM">code]</a>, <i>Journal of Computational Physics, 538:114163, 2025.</i> </p>
<ul>
<li><p>Bowen Li and <b>Bin Shi</b>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2212.06319">Linear Convergence of ISTA and FISTA</a>, <i>Journal of the Operations Research Society of China, 2024, Published online.</i></p>
<ul>
<li><p>Bowen Li, <b>Bin Shi</b> and Ya-xiang Yuan</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://jmlr.org/papers/v25/22-1189.html">On the Hyperparameters in Stochastic Gradient Descent with Momentum</a>, <i>Journal of Machine Learning Research, 25(236):1-40, 2024.</i> </p>
<ul>
<li><p><b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://epubs.siam.org/doi/abs/10.1137/23M158111X">Linear Convergence of Forward-Backward Accelerated Algorithms without Knowledge of the Modulus of Strong Convexity</a>, <i>SIAM Journal on Optimization, 34(2):2150-2168, 2024.</i></p>
<ul>
<li><p>Bowen Li, <b>Bin Shi</b> and Ya-xiang Yuan</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://npg.copernicus.org/articles/31/165/2024/">The sampling method for optimal precursors of El Niño-Southern Oscillation events</a>,  <i>Nonlinear Processes in Geophysics, 31(1):165–174, 2024.</i></p>
<ul>
<li><p><b>Bin Shi</b> and Junjie Ma </p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://jmlr.org/papers/v24/20-364.html">On Learning Rates and Schrödinger Operators</a>,  <i>Journal of Machine Learning Research, 24(379):1-53, 2023.</i></p>
<ul>
<li><p><b>Bin Shi</b>, Weijie Su and Michael I. Jordan</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://npg.copernicus.org/articles/30/263/2023/">An adjoint-free algorithm for conditional nonlinear optimal perturbations (CNOPs) via sampling</a>, <i>Nonlinear Processes in Geophysics, 30(3):263–276, 2023.</i></p>
<ul>
<li><p><b>Bin Shi</b> and Guodong Sun</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10107-021-01681-8">Understanding the Acceleration Phenomenon via High-Resolution Differential Equations</a>,   <i>Mathematical Programming, Series A, 195(1):79–148, 2022.</i></p>
<ul>
<li><p><b>Bin Shi</b>, Simon S. Du, Weijie Su and Michael I. Jordan</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s40316-021-00176-4">Conjugate and Cut Points in Ideal Fluid Motion</a>, <i>Annales Mathématiques du Québec, 46(1):207-225, 2022.</i></p>
<ul>
<li><p>Theodore D. Drivas, Gerard Misiołek, <b>Bin Shi</b> and Tsuyoshi Yoneda</p>
</li>
</ul>

</li>
</ul>
<h2>Workshop and Conference Proceedings</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/1902.03694">Quantum Optimization via Gradient-Based Hamiltonian Descent</a>,&nbsp;&nbsp;[<a href="https://github.com/jiaqileng/Gradient-Based-QHD">code]</a>, <i>ICML 2025.</i> </p>
<ul>
<li><p>Jiaqi Leng, <b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1902.03694">Acceleration via Symplectic Discretization of High-Resolution Differential Equations</a>, <i>NeurIPS 2019.</i> </p>
<ul>
<li><p><b>Bin Shi</b>, Simon S. Du, Weijie J. Su and Michael I. Jordan</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1708.08035">A Conservation Law Method in Optimization</a>, <i><a href="https://opt-ml.org/oldopt/opt17/">The Tenth Workshop on Optimization for Machine Learning</a>, </i>OptML Workshop, NeurIPS 2017./</p>
<ul>
<li><p><b>Bin Shi</b>, Tao Li and Sundaraja S. Iyengar</p>
</li>
</ul>

</li>
</ul>
<h2>Monograph </h2>
<ul>
<li><p><a href="https://www.springer.com/gp/book/9783030170752">Mathematical Theories of Machine Learning - Theory and Applications</a>, <i>Springer International Publishing, 2020.</i></p>
<ul>
<li><p><b>Bin Shi</b> and Sundaraja S. Iyengar</p>
</li>
</ul>

</li>
</ul>
<h2>Preprints </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2209.08862">Gradient Norm Minimization of Nesterov's Acceleration o(1/k^3)</a>,</p>
<ul>
<li><p>Shuo Chen, <b>Bin Shi</b> and Ya-xiang Yuan
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.06011">Optimal Disturbances of Blocking: A Barotropic View</a>,</p>
<ul>
<li><p><b>Bin Shi</b>, Dehai Luo and Wenqi Zhang
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2211.01610">Proximal Subgradient Norm Minimization of ISTA and FISTA</a>,</p>
<ul>
<li><p>Bowen Li, <b>Bin Shi</b> and Ya-xiang Yuan
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2212.05700">Revisiting the acceleration phenomenon via high-resolution differential equations</a>,</p>
<ul>
<li><p>Shuo Chen, <b>Bin Shi</b> and Ya-xiang Yuan
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2304.14642">On Underdamped Nesterov's Acceleration</a>,</p>
<ul>
<li><p>Shuo Chen, <b>Bin Shi</b> and Ya-xiang Yuan
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2401.07096">Understanding the ADMM Algorithm via High-Resolution Differential Equations</a>, </p>
<ul>
<li><p>Bowen Li and <b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2403.11139">Understanding the PDHG Algorithm via High-Resolution Differential Equations</a>, </p>
<ul>
<li><p>Bowen Li and <b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2412.13527">Lyapunov Analysis For Monotonically Forward-Backward Accelerated Algorithms</a>,</p>
<ul>
<li><p>Mingwei Fu and <b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2501.10051">A Family of Controllable Momentum Coefficients for Forward-Backward Accelerated Algorithms</a>,</p>
<ul>
<li><p>Mingwei Fu and <b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.10896">On Pseudospectral Concentration for Rank-1 Sampling</a>,</p>
<ul>
<li><p>Kuo Gai and <b>Bin Shi</b></p>
</li>
</ul>

</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
